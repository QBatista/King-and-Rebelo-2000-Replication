{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import quantecon\n",
    "import quantecon.game_theory as gt\n",
    "from scipy.stats import rv_discrete\n",
    "\n",
    "g_PD = gt.NormalFormGame((2, 2))\n",
    "g_PD[0, 0] = 1, 1\n",
    "g_PD[0, 1] = -2, 3\n",
    "g_PD[1, 0] = 3, -2\n",
    "g_PD[1, 1] = 0, 0\n",
    "\n",
    "g_BoS = gt.NormalFormGame((2, 2))\n",
    "g_BoS[0, 0] = 4, 3\n",
    "g_BoS[0, 1] = 1, 1\n",
    "g_BoS[1, 0] = 0, 0\n",
    "g_BoS[1, 1] = 3, 4\n",
    "\n",
    "g_MP = gt.NormalFormGame((2, 2))\n",
    "g_MP[0, 0] = -2, 2\n",
    "g_MP[0, 1] = 2, -2\n",
    "g_MP[1, 0] = 2, -2\n",
    "g_MP[1, 1] = -2, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([ 0.,  1.]), array([ 0.,  1.]))]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt.support_enumeration(g_PD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([ 1.,  0.]), array([ 1.,  0.])),\n",
       " (array([ 0.,  1.]), array([ 0.,  1.])),\n",
       " (array([ 0.57142857,  0.42857143]), array([ 0.42857143,  0.57142857]))]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt.support_enumeration(g_BoS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([ 0.5,  0.5]), array([ 0.5,  0.5]))]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt.support_enumeration(g_MP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "- Allow any initial weights to be specified explicitly\n",
    "- Introduce exploration probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_network_graph(action_space, initial_weights, optimizer):\n",
    "    \"\"\"Create the tensorflow graph for an agent\"\"\"\n",
    "    weights = tf.nn.softmax(tf.Variable(initial_weights)) \n",
    "\n",
    "    reward_holder = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "    action_holder = tf.placeholder(shape=[1], dtype=tf.int32)\n",
    "    responsible_weight = tf.slice(weights, action_holder, [1])\n",
    "\n",
    "    loss = -(tf.log(responsible_weight)*reward_holder)\n",
    "    update = optimizer.minimize(loss)\n",
    "\n",
    "    sess = tf.Session()\n",
    "\n",
    "    return (weights, reward_holder, action_holder, responsible_weight, loss,\n",
    "            update, sess)\n",
    "\n",
    "\n",
    "def play_action(game, player, p1_action, p2_action):\n",
    "    '''Play the game once an action has been decided by player 2'''\n",
    "    return game.payoff_profile_array[p1_action][p2_action][player]\n",
    "\n",
    "\n",
    "def choose_probabilistic_action(action_space, density):\n",
    "    '''Choose an action from the action space according to a specified\n",
    "        density'''\n",
    "    distribution = rv_discrete(values=(action_space, density))\n",
    "    return distribution.rvs()\n",
    "\n",
    "\n",
    "def play_game_nvh(game, initial_weights, opponent_actions, opponent_density,\n",
    "                  episodes=100000, verbose=True):\n",
    "    '''Play game between a network and a hard-coded agent.'''\n",
    "    # Graph\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    action_space = game.nums_actions[0]\n",
    "\n",
    "    weights, rholder, aholder, rweights, loss, update, sess = \\\n",
    "        create_network_graph(action_space, initial_weights,\n",
    "                             tf.train.AdamOptimizer(learning_rate=0.001))\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Launch the tensorflow graph\n",
    "    sess.run(init)\n",
    "    for i in range(episodes):\n",
    "        if i == 0:\n",
    "            action = choose_probabilistic_action([0, 1], initial_weights)\n",
    "        else:\n",
    "            action = choose_probabilistic_action([0, 1], updated_weights)\n",
    "        opponent_action = choose_probabilistic_action(opponent_actions,\n",
    "                                                      opponent_density)\n",
    "        reward = play_action(game, 0, action, opponent_action)\n",
    "\n",
    "        # Update the network.\n",
    "        _, resp, updated_weights = sess.run([update, rweights, weights],\n",
    "                                            feed_dict={rholder: [reward],\n",
    "                                                       aholder: [action]})\n",
    "        \n",
    "        if verbose:\n",
    "            if i % 5000 == 0:\n",
    "                print(\"Weights at step \" + str(i) + \": \" +\n",
    "                      str(updated_weights))\n",
    "                print(\"Reward: \" + str(reward))\n",
    "\n",
    "    return updated_weights\n",
    "\n",
    "\n",
    "def play_game_nvn(game, initial_weights1, initial_weights2, episodes=50000, verbose=True):\n",
    "    '''Play game between two networks.'''\n",
    "    # Graph\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    action_space = game.nums_actions[0]\n",
    "\n",
    "    weights1, rholder1, aholder1, rweights1, loss1, update1, sess1 = \\\n",
    "        create_network_graph(action_space, initial_weights1,\n",
    "                             tf.train.GradientDescentOptimizer(learning_rate=0.001))\n",
    "    weights2, rholder2, aholder2, rweights2, loss2, update2, sess2 = \\\n",
    "        create_network_graph(action_space, initial_weights2,\n",
    "                             tf.train.GradientDescentOptimizer(learning_rate=0.001))\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Launch the tensorflow graph\n",
    "    sess1.run(init)\n",
    "    sess2.run(init)\n",
    "    for i in range(episodes):\n",
    "        if i == 0:\n",
    "            action1 = choose_probabilistic_action([0, 1], initial_weights1)\n",
    "            action2 = choose_probabilistic_action([0, 1], initial_weights2)\n",
    "        else:\n",
    "            action1 = choose_probabilistic_action([0, 1], updated_weights_1)\n",
    "            action2 = choose_probabilistic_action([0, 1], updated_weights_2)\n",
    "\n",
    "        reward1 = play_action(game, 0, action1, action2)\n",
    "        reward2 = play_action(game, 1, action1, action2)\n",
    "\n",
    "        # Update the network.\n",
    "        _, resp, updated_weights_1 = sess1.run([update1, rweights1, weights1],\n",
    "                                               feed_dict={rholder1: [reward1],\n",
    "                                                          aholder1: [action1]})\n",
    "\n",
    "        _, resp, updated_weights_2 = sess2.run([update2, rweights2, weights2],\n",
    "                                               feed_dict={rholder2: [reward2],\n",
    "                                                          aholder2: [action2]})\n",
    "\n",
    "        if verbose:\n",
    "            if i % 5000 == 0:\n",
    "                print(\"Weights at step \" + str(i) + \": (P1)\" +\n",
    "                      str(updated_weights_1) + \" (P2)\" +\n",
    "                      str(updated_weights_2))\n",
    "                print(\"Rewards: \" + str([reward1, reward2]))\n",
    "\n",
    "    return updated_weights_1, updated_weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights at step 0: (P1)[ 0.5  0.5] (P2)[ 0.5  0.5]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 5000: (P1)[ 0.49286744  0.50713259] (P2)[ 0.5039115   0.49608847]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 10000: (P1)[ 0.47996759  0.52003241] (P2)[ 0.4630836   0.53691632]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 15000: (P1)[ 0.46358716  0.53641284] (P2)[ 0.52080208  0.47919789]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 20000: (P1)[ 0.51972896  0.4802711 ] (P2)[ 0.52648401  0.47351596]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 25000: (P1)[ 0.58781433  0.41218573] (P2)[ 0.49656159  0.50343847]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 30000: (P1)[ 0.47883213  0.52116781] (P2)[ 0.39453125  0.60546881]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 35000: (P1)[ 0.37557551  0.62442446] (P2)[ 0.48321331  0.51678663]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 40000: (P1)[ 0.44807091  0.55192912] (P2)[ 0.64011198  0.35988802]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 45000: (P1)[ 0.62773943  0.3722606 ] (P2)[ 0.53446293  0.46553716]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 50000: (P1)[ 0.52334625  0.47665378] (P2)[ 0.4537788   0.54622126]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 55000: (P1)[ 0.52210325  0.47789675] (P2)[ 0.4396731   0.56032699]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 60000: (P1)[ 0.42007113  0.57992882] (P2)[ 0.46496442  0.53503555]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 65000: (P1)[ 0.47848743  0.52151257] (P2)[ 0.55847842  0.44152158]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 70000: (P1)[ 0.58881241  0.41118762] (P2)[ 0.6184454   0.38155466]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 75000: (P1)[ 0.64954478  0.35045522] (P2)[ 0.41628343  0.58371657]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 80000: (P1)[ 0.44561571  0.55438429] (P2)[ 0.36295366  0.63704634]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 85000: (P1)[ 0.33531716  0.66468287] (P2)[ 0.58880568  0.41119432]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 90000: (P1)[ 0.5860936   0.41390634] (P2)[ 0.66581482  0.33418521]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 95000: (P1)[ 0.66098666  0.33901331] (P2)[ 0.39143676  0.60856318]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 100000: (P1)[ 0.36866719  0.63133287] (P2)[ 0.40350318  0.59649676]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 105000: (P1)[ 0.38497904  0.61502099] (P2)[ 0.5607478   0.43925217]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 110000: (P1)[ 0.55930954  0.44069049] (P2)[ 0.59549195  0.40450802]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 115000: (P1)[ 0.57171965  0.42828035] (P2)[ 0.51284897  0.487151  ]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 120000: (P1)[ 0.54356349  0.45643649] (P2)[ 0.4419924  0.5580076]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 125000: (P1)[ 0.40261468  0.59738523] (P2)[ 0.38176259  0.61823744]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 130000: (P1)[ 0.34996966  0.65003037] (P2)[ 0.56216627  0.43783376]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 135000: (P1)[ 0.5742057   0.42579433] (P2)[ 0.65869856  0.34130147]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 140000: (P1)[ 0.66487348  0.33512643] (P2)[ 0.44946763  0.55053234]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 145000: (P1)[ 0.48230213  0.51769787] (P2)[ 0.30213243  0.69786757]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 150000: (P1)[ 0.26537633  0.73462367] (P2)[ 0.58181477  0.41818526]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 155000: (P1)[ 0.59142113  0.40857884] (P2)[ 0.71862411  0.28137589]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 160000: (P1)[ 0.66013891  0.33986112] (P2)[ 0.31978798  0.68021202]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 165000: (P1)[ 0.25084805  0.74915195] (P2)[ 0.36708447  0.63291556]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 170000: (P1)[ 0.53303993  0.46696007] (P2)[ 0.78207546  0.21792452]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 175000: (P1)[ 0.73169869  0.26830131] (P2)[ 0.24599004  0.75400996]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 180000: (P1)[ 0.20715205  0.79284793] (P2)[ 0.57877034  0.42122966]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 185000: (P1)[ 0.73121089  0.26878905] (P2)[ 0.7174598  0.2825402]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 190000: (P1)[ 0.53617382  0.46382621] (P2)[ 0.22015287  0.77984715]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 195000: (P1)[ 0.25346678  0.74653316] (P2)[ 0.59985292  0.40014711]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 200000: (P1)[ 0.7824052   0.21759482] (P2)[ 0.6688329   0.33116707]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 205000: (P1)[ 0.41051584  0.58948416] (P2)[ 0.22518143  0.77481866]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 210000: (P1)[ 0.32394385  0.67605615] (P2)[ 0.73884201  0.26115805]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 215000: (P1)[ 0.80990595  0.19009408] (P2)[ 0.48305985  0.51694006]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 220000: (P1)[ 0.23468281  0.7653172 ] (P2)[ 0.28847301  0.71152699]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 225000: (P1)[ 0.63976914  0.36023092] (P2)[ 0.82086587  0.17913413]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 230000: (P1)[ 0.57549804  0.42450199] (P2)[ 0.15378468  0.84621537]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 235000: (P1)[ 0.31253546  0.68746448] (P2)[ 0.79853493  0.20146506]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 240000: (P1)[ 0.80312985  0.19687013] (P2)[ 0.24597228  0.75402772]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 245000: (P1)[ 0.16440722  0.83559281] (P2)[ 0.61612445  0.38387564]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 250000: (P1)[ 0.83033615  0.16966383] (P2)[ 0.58113569  0.41886431]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 255000: (P1)[ 0.20869738  0.79130256] (P2)[ 0.27189881  0.72810119]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 260000: (P1)[ 0.70565677  0.29434323] (P2)[ 0.82038307  0.17961693]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 265000: (P1)[ 0.45666036  0.54333961] (P2)[ 0.17131093  0.82868904]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 270000: (P1)[ 0.31813532  0.68186474] (P2)[ 0.80068183  0.1993182 ]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 275000: (P1)[ 0.80224997  0.19775   ] (P2)[ 0.29994363  0.70005643]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 280000: (P1)[ 0.16015926  0.83984071] (P2)[ 0.51191914  0.48808086]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 285000: (P1)[ 0.82579517  0.17420487] (P2)[ 0.70324081  0.29675916]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 290000: (P1)[ 0.23804708  0.76195294] (P2)[ 0.20844531  0.79155463]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 295000: (P1)[ 0.58434075  0.41565925] (P2)[ 0.83624595  0.16375402]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 300000: (P1)[ 0.65833682  0.34166315] (P2)[ 0.19364926  0.80635077]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 305000: (P1)[ 0.20355785  0.79644215] (P2)[ 0.70157003  0.29842991]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 310000: (P1)[ 0.79943049  0.20056947] (P2)[ 0.53154409  0.46845585]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 315000: (P1)[ 0.31218109  0.68781894] (P2)[ 0.23781261  0.76218736]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 320000: (P1)[ 0.43105215  0.56894785] (P2)[ 0.78597784  0.21402219]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 325000: (P1)[ 0.75824058  0.24175943] (P2)[ 0.33325011  0.66674989]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 330000: (P1)[ 0.24810992  0.75189012] (P2)[ 0.38018414  0.61981583]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 335000: (P1)[ 0.50196064  0.49803934] (P2)[ 0.71129888  0.28870115]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 340000: (P1)[ 0.69296771  0.30703223] (P2)[ 0.49216852  0.50783145]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 345000: (P1)[ 0.470231  0.529769] (P2)[ 0.33556923  0.6644308 ]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 350000: (P1)[ 0.37005046  0.62994957] (P2)[ 0.57280308  0.42719692]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 355000: (P1)[ 0.58630461  0.41369545] (P2)[ 0.62314999  0.37684998]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 360000: (P1)[ 0.65169698  0.34830311] (P2)[ 0.43112934  0.56887066]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 365000: (P1)[ 0.4808822   0.51911783] (P2)[ 0.33356079  0.66643924]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 370000: (P1)[ 0.41094953  0.58905047] (P2)[ 0.55555463  0.44444537]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 375000: (P1)[ 0.57114589  0.42885405] (P2)[ 0.62987107  0.3701289 ]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 380000: (P1)[ 0.67099482  0.32900524] (P2)[ 0.46943122  0.53056878]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 385000: (P1)[ 0.43994805  0.56005204] (P2)[ 0.3097949  0.6902051]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 390000: (P1)[ 0.3014493  0.6985507] (P2)[ 0.59832644  0.40167356]\n",
      "Rewards: [-2.0, 2.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights at step 395000: (P1)[ 0.65308058  0.34691939] (P2)[ 0.66722208  0.33277792]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 400000: (P1)[ 0.60174042  0.39825958] (P2)[ 0.34144095  0.65855902]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 405000: (P1)[ 0.35914078  0.64085925] (P2)[ 0.40862811  0.59137189]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 410000: (P1)[ 0.41578582  0.58421421] (P2)[ 0.64625746  0.35374257]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 415000: (P1)[ 0.64130658  0.35869345] (P2)[ 0.6046477   0.39535224]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 420000: (P1)[ 0.59000951  0.40999046] (P2)[ 0.36887807  0.63112193]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 425000: (P1)[ 0.37912878  0.62087119] (P2)[ 0.38598493  0.61401498]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 430000: (P1)[ 0.40165657  0.59834337] (P2)[ 0.54793429  0.45206565]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 435000: (P1)[ 0.56630737  0.43369263] (P2)[ 0.61837524  0.38162476]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 440000: (P1)[ 0.55007452  0.44992548] (P2)[ 0.50526178  0.49473819]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 445000: (P1)[ 0.50783283  0.4921672 ] (P2)[ 0.41064247  0.5893575 ]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 450000: (P1)[ 0.37413162  0.62586838] (P2)[ 0.48131281  0.51868719]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 455000: (P1)[ 0.40955263  0.59044731] (P2)[ 0.66267365  0.33732641]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 460000: (P1)[ 0.68297696  0.31702301] (P2)[ 0.53850222  0.46149781]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 465000: (P1)[ 0.47033092  0.52966905] (P2)[ 0.34294125  0.65705872]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 470000: (P1)[ 0.32826373  0.6717363 ] (P2)[ 0.49330845  0.50669158]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 475000: (P1)[ 0.51012576  0.48987427] (P2)[ 0.6872223   0.31277764]\n",
      "Rewards: [2.0, -2.0]\n",
      "Weights at step 480000: (P1)[ 0.68117452  0.31882548] (P2)[ 0.46274352  0.53725648]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 485000: (P1)[ 0.36694881  0.63305116] (P2)[ 0.30914995  0.69085002]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 490000: (P1)[ 0.34389827  0.6561017 ] (P2)[ 0.69344199  0.30655795]\n",
      "Rewards: [-2.0, 2.0]\n",
      "Weights at step 495000: (P1)[ 0.67607212  0.32392791] (P2)[ 0.58603907  0.41396096]\n",
      "Rewards: [-2.0, 2.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 0.59512514,  0.40487483], dtype=float32),\n",
       " array([ 0.31605208,  0.68394792], dtype=float32))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_game_nvn(g_MP, [0.5, 0.5], [0.5, 0.5], episodes=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights at step 0: [ 0.5  0.5]\n",
      "Reward: 1.0\n",
      "Weights at step 5000: [ 0.03577258  0.96422744]\n",
      "Reward: 3.0\n",
      "Weights at step 10000: [ 0.00608065  0.99391937]\n",
      "Reward: 3.0\n",
      "Weights at step 15000: [ 0.00210897  0.99789101]\n",
      "Reward: 3.0\n",
      "Weights at step 20000: [ 0.00109224  0.9989078 ]\n",
      "Reward: 3.0\n",
      "Weights at step 25000: [  4.82685689e-04   9.99517322e-01]\n",
      "Reward: 3.0\n",
      "Weights at step 30000: [  9.99089752e-05   9.99900103e-01]\n",
      "Reward: 3.0\n",
      "Weights at step 35000: [  1.03530010e-05   9.99989629e-01]\n",
      "Reward: 3.0\n",
      "Weights at step 40000: [  9.77906780e-07   9.99999046e-01]\n",
      "Reward: 3.0\n",
      "Weights at step 45000: [  8.50487822e-08   9.99999881e-01]\n",
      "Reward: 3.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  1.58143063e-08,   1.00000000e+00], dtype=float32)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_game_nvh(g_BoS, [0.5, 0.5], [0, 1], [0, 1], episodes=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import ColumnDataSource, LabelSet, Legend, HoverTool\n",
    "from bokeh.palettes import Category20\n",
    "\n",
    "data = pd.DataFrame(hist).T\n",
    "data = data.cumsum().div(pd.Series(data.index)+1, axis='index')\n",
    "\n",
    "TOOLS = \"crosshair,pan,wheel_zoom,reset,tap,save,box_select,hover\"\n",
    "\n",
    "numlines = len(data.columns)\n",
    "#mypalette = Category20[numlines]\n",
    "\n",
    "p = figure(tools=TOOLS, plot_width=800, plot_height=700)\n",
    "\n",
    "p.multi_line(xs=[data.index.values]*numlines,\n",
    "             ys=[data[name].values for name in data])\n",
    "\n",
    "p.title.text = \"Evolution of SGD Agent Best Response Play\"\n",
    "p.title.text_font_size = \"20px\"\n",
    "\n",
    "p.yaxis.axis_label = 'Cumulative Frequency'\n",
    "p.xaxis.axis_label = 'Episodes'\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
